# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/189R-4FKh9tbRVQ_AtYaSXin1JS2bujIs
"""

import pandas as pd
import numpy as np

data = pd.read_csv("mhr.csv")
data.head()

from sklearn import  preprocessing
le= preprocessing.LabelEncoder()

for C in data.columns:
  data[C] = le.fit_transform(data[C])

data.head()

import seaborn as sns 
import matplotlib.pyplot as plt
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score

def Değişim(isim, eski, yeni):
  for a in dataset.columns:
    if a == isim:
      for sutun in range(0, 8124):       
        if eski == dataset.loc[sutun,a]:
          dataset.loc[sutun,a] = yeni

Değişim("bruises", 0, 0)

plt.figure(figsize=(15,10))
sns.heatmap(new_data.corr(),annot=True,linewidths=0.5,cmap="Purples")

X = data.iloc[:, 1:24].values
Y = data.iloc[:, 0].values

#scaler = StandardScaler()
#X = scaler.fit_transform(X)

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state = 40)

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

X = data.iloc[:, 1:24]
Y = data.iloc[:, 0]

bestfeatures = SelectKBest(score_func=chi2, k=10)
fit = bestfeatures.fit(X,Y)
dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(X.columns)

featureScores = pd.concat([dfcolumns,dfscores],axis=1)
featureScores.columns = ['Specs','Score'] 
print(featureScores.nlargest(10,'Score'))

from sklearn.ensemble import ExtraTreesClassifier
import matplotlib.pyplot as plt
model = ExtraTreesClassifier()
model.fit(X,Y)
print(model.feature_importances_) 
feat_importances = pd.Series(model.feature_importances_, index=X.columns)
feat_importances.nlargest(10).plot(kind='barh')
plt.show()

new_data = data[["gill-color", "ring-type", "gill-size", "bruises", "stalk-root", "gill-spacing",
                "habitat", "spore-print-color", "population", "stalk-surface-above-ring"]]

from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier

embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=10)
embeded_rf_selector.fit(X, Y)

embeded_rf_support = embeded_rf_selector.get_support()
embeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist()
print(str((embeded_rf_feature)), 'selected features')

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.preprocessing import MinMaxScaler
X_norm = MinMaxScaler().fit_transform(X)
chi_selector = SelectKBest(chi2, k=10)
chi_selector.fit(X_norm, Y)
chi_support = chi_selector.get_support()
chi_feature = X.loc[:,chi_support].columns.tolist()
print(str((chi_feature)), 'selected features')

from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=10, step=10, verbose=5)
rfe_selector.fit(X_norm, Y)
rfe_support = rfe_selector.get_support()
rfe_feature = X.loc[:,rfe_support].columns.tolist()
print((rfe_feature), 'selected features')

from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LogisticRegression

embeded_lr_selector = SelectFromModel(LogisticRegression(penalty="l2"), max_features=10)
embeded_lr_selector.fit(X_norm, Y)

embeded_lr_support = embeded_lr_selector.get_support()
embeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist()
print(embeded_lr_feature, 'selected features')

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from xgboost import XGBRFClassifier

Algoritmalar = [LogisticRegression,
                 GaussianNB,
                 KNeighborsClassifier,
                 DecisionTreeClassifier,
                 XGBClassifier,
                 XGBRFClassifier,
                 RandomForestClassifier]

def Deney(alg):
  X = new_data
  y = data.iloc[:, 0].values

  scaler = StandardScaler()
  X = scaler.fit_transform(X)

  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

  algs=alg().fit(X_train,y_train)
  y_pred = algs.predict(X_test)

  algoritma_ismi = alg.__name__
  from sklearn.metrics import f1_score
  print(algoritma_ismi, "SCORE==>", f1_score(y_test, y_pred, average=None))
  print("")

for i in Algoritmalar:
  Deney(i)

"""LogisticRegression SCORE==> [0.95302548 0.9500846 ]

GaussianNB SCORE==> [0.93163752 0.92711864]

KNeighborsClassifier SCORE==> [1. 1.]

DecisionTreeClassifier SCORE==> [1. 1.]

XGBClassifier SCORE==> [1. 1.]

XGBRFClassifier SCORE==> [0.98120595 0.97932817]

RandomForestClassifier SCORE==> [1. 1.]
"""

